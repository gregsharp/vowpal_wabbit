enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 10
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/rcv1_small.dat.cache
Reading from train-sets/rcv1_small.dat
num sources = 1
 1 6.931472e-01	1.859805e-03	3.768599e+00	          	          	          	3.414409e+01	1.977478e+04	1.103734e-01	2.108
 3 4.624629e-01	1.085868e-02	1.936935e+00	 0.554592  	0.194583  	          	          	2.208660e+02	1.000000e+00	2.607     
 4 3.384483e-01	4.246157e-04	1.726574e-01	 0.520733  	0.133178  	          	          	3.415247e+01	1.000000e+00	3.199     
 5 3.184994e-01	7.930477e-05	6.343691e-02	 0.751656  	0.517330  	          	          	6.775453e+01	1.000000e+00	3.867     
 6 3.024582e-01	2.887344e-06	1.460970e-02	 0.657158  	0.328843  	          	          	3.618752e+01	1.000000e+00	4.613     
 7 2.967627e-01	4.279802e-06	3.517537e-03	 0.645592  	0.293725  	          	          	1.585137e+01	1.000000e+00	5.437     
 8 2.952703e-01	2.241755e-06	1.631614e-03	 0.527831  	0.052654  	          	          	5.802390e+00	1.000000e+00	6.347     
 9 2.950591e-01	7.095166e-07	1.546886e-03	 0.199872  	-0.608863 	          	          	7.389087e-01	1.000000e+00	7.330     

finished run
number of examples = 200000
weighted example sum = 2e+05
weighted label sum = -1.272e+04
average loss = 0.2911
best constant = -0.06361
total feature number = 15587880
10 2.948347e-01	2.803435e-07	1.223611e-04	 0.585419  	0.171988  	          	          	1.453111e-01	1.000000e+00	10.251    

