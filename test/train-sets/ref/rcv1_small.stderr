enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 0.5
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization = 1
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/rcv1_small.dat.cache
Reading from train-sets/rcv1_small.dat
num sources = 1
 1 0.69315   	0.01247   	109.37472 	          	          	          	165.25443 	1072660.62500	0.66186   	0.274
 3 24.96379  	0.09448   	58.49782  	 -0.335275 	-0.667864 	          	          	(revise x 0.5)	0.33093   	0.306
 4 6.68890   	0.07916   	21.38882  	 -0.165651 	-0.341257 	          	          	(revise x 0.5)	0.16546   	0.344
 5 2.02595   	0.05334   	9.61256   	 -0.073645 	-0.172136 	          	          	(revise x 0.5)	0.08273   	0.381
 6 0.88609   	0.02271   	4.21489   	 -0.021323 	-0.077380 	          	          	(revise x 0.5)	0.04137   	0.417
 7 0.65787   	0.00567   	2.42293   	 0.007797  	-0.022375 	          	          	(revise x 0.5)	0.02068   	0.454
 8 0.64090   	0.00109   	2.53848   	 0.023094  	0.007617  	          	          	5.59308   	1.00000   	0.567     
 9 0.59157   	0.00084   	1.98471   	 0.939814  	0.880475  	          	          	449.76352 	1.00000   	0.700     
10 0.36198   	0.00015   	0.22195   	 0.590336  	0.255459  	          	          	123.06593 	1.00000   	0.854     
11 0.34817   	0.00506   	0.68001   	 0.218977  	-0.427065 	          	          	27.43107  	1.00000   	1.028     
12 0.32328   	0.00029   	0.08338   	 0.597309  	0.217820  	          	          	3.59894   	1.00000   	1.223     
13 0.31983   	0.00001   	0.04912   	 0.738430  	0.480967  	          	          	8.91300   	1.00000   	1.439     
14 0.31597   	0.00006   	0.04996   	 0.762546  	0.528461  	          	          	24.29691  	1.00000   	1.674     
15 0.31083   	0.00018   	0.05458   	 0.726800  	0.455732  	          	          	103.02377 	1.00000   	1.910     
16 0.29800   	0.00004   	0.01069   	 0.692397  	0.384367  	          	          	50.95499  	1.00000   	2.147     
17 0.29486   	0.00000   	0.00018   	 0.516694  	0.031775  	          	          	0.39482   	1.00000   	2.384     
18 0.29479   	0.00000   	0.00009   	 0.595950  	0.192321  	          	          	0.24618   	1.00000   	2.621     

finished run
number of examples = 200000
weighted example sum = 2e+05
weighted label sum = -1.272e+04
average loss = 0.4485
best constant = -0.06361
total feature number = 15587880
19 0.29476   	0.00000   	0.00003   	 0.619564  	0.238334  	          	          	0.15642   	1.00000   	2.884     

