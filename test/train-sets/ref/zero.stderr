enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 0.5
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization = 1
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/zero.dat.cache
Reading from train-sets/zero.dat
num sources = 1
 1 0.00000   	41943.04000	209549.29466	          	          	          	223389.60318	1047595.31250	0.93804   	0.059
 3 92180.71744	36872.28737	184355.06675	 -0.468954 	-0.937909 	          	          	(revise x 0.5)	0.46902   	0.070
 4 23045.17936	9218.07184	46088.76669	 -0.234477 	-0.468954 	          	          	(revise x 0.5)	0.23451   	0.086

finished run
number of examples = 25
weighted example sum = 25
weighted label sum = 0
average loss = 0
best constant = -0.04167
total feature number = 15005
 5 5761.29484	2304.51796	11522.19167	 -0.117239 	-0.234477 	          	          	(revise x 0.5)	0.11726   	0.101

